{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bb9698",
   "metadata": {},
   "source": [
    "# Flightops - Data Acquisition, Loading and Pre-processing\n",
    "\n",
    "## Extracting Data from BTS\n",
    "Flight data for 2018‚Äì2019 is sourced from the BTS TranStats repository using an automated downloader script:\n",
    "- `download_bts_ontime.py` fetches monthly On-Time Performance files and extracts them into the project directory.\n",
    "- Run this command from the repo root to download the necessary files: *python notebooks/download_bts_ontime.py*\n",
    "\n",
    "## Load and Clean Data\n",
    "\n",
    "Data cleaning is one of the most critical (and usually most tedious) part of data analysis. But the effort is always worth it because, as the old saying goes, **garbage in, garbage out!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f86cb1",
   "metadata": {},
   "source": [
    "environment: conda activate datasci311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c3b85",
   "metadata": {},
   "source": [
    "### Set stable root directory for relative paths in VSCode\n",
    "\n",
    "VS Code + Jupyter has notoriously unstable CWDs because it is flexible in where it runs notebooks from (e.g. the workspace root, a previously cached directory, a kernel opened *before* the repo was opened...). It's great that VSC has this flexibility, but it presents a weakness when making repos available in CI/GitHub. I've removed the ambiguity by controlling the CWD to make relative paths stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Establish repo root (notebooks live one level down)\n",
    "REPO_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data/raw/csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e10418",
   "metadata": {},
   "source": [
    "### Load and Concatenate Files\n",
    "\n",
    "After extracting the required files from BTS, we'll load and combine them into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "print(f\"Found {len(files)} CSV files.\") # used for visibility when debugging cwd issues\n",
    "\n",
    "# Specify columns to load to save memory, reducing columns loaded from 110 to 30 and dataframe memory usage from ~24.8 GB to ~11.1 GB\n",
    "usecols = [\n",
    "    # Date / identifiers\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"DayofMonth\",\n",
    "    \"DayOfWeek\",\n",
    "    \"FlightDate\",\n",
    "    \"Reporting_Airline\",\n",
    "    \"IATA_CODE_Reporting_Airline\",\n",
    "    \"Flight_Number_Reporting_Airline\",\n",
    "    \"Tail_Number\",\n",
    "\n",
    "    # Origin / destination\n",
    "    \"Origin\",\n",
    "    \"OriginCityName\",\n",
    "    \"OriginState\",\n",
    "    \"Dest\",\n",
    "    \"DestCityName\",\n",
    "    \"DestState\",\n",
    "\n",
    "    # Scheduled vs actual times\n",
    "    \"CRSDepTime\",\n",
    "    \"CRSArrTime\",\n",
    "    \"DepTime\",\n",
    "    \"ArrTime\",\n",
    "\n",
    "    # Delay metrics\n",
    "    \"DepDelay\",\n",
    "    \"DepDelayMinutes\",\n",
    "    \"DepDel15\",\n",
    "    \"ArrDelay\",\n",
    "    \"ArrDelayMinutes\",\n",
    "    \"ArrDel15\",\n",
    "\n",
    "    # Status flags\n",
    "    \"Cancelled\",\n",
    "    \"Diverted\",\n",
    "    \"CancellationCode\",\n",
    "\n",
    "    # Distance / throughput\n",
    "    \"Distance\",\n",
    "    \"Flights\",\n",
    "]\n",
    "\n",
    "# Adding a dtype map to reduce memory usage further\n",
    "dtypes = {\n",
    "    \"Year\": \"int16\",\n",
    "    \"Month\": \"int8\",\n",
    "    \"DayofMonth\": \"int8\",\n",
    "    \"DayOfWeek\": \"int8\",\n",
    "    \"Reporting_Airline\": \"category\",\n",
    "    \"IATA_CODE_Reporting_Airline\": \"category\",\n",
    "    \"Tail_Number\": \"category\",\n",
    "    \"Flight_Number_Reporting_Airline\": \"int32\",\n",
    "    \"Origin\": \"category\",\n",
    "    \"OriginCityName\": \"category\",\n",
    "    \"OriginState\": \"category\",\n",
    "    \"Dest\": \"category\",\n",
    "    \"DestCityName\": \"category\",\n",
    "    \"DestState\": \"category\",\n",
    "    \"CRSDepTime\": \"int32\",\n",
    "    \"CRSArrTime\": \"int32\",\n",
    "    \"DepTime\": \"float32\",\n",
    "    \"ArrTime\": \"float32\",\n",
    "    \"DepDelay\": \"float32\",\n",
    "    \"DepDelayMinutes\": \"float32\",\n",
    "    \"DepDel15\": \"Int8\",\n",
    "    \"ArrDelay\": \"float32\",\n",
    "    \"ArrDelayMinutes\": \"float32\",\n",
    "    \"ArrDel15\": \"Int8\",\n",
    "    \"Cancelled\": \"Int8\",\n",
    "    \"Diverted\": \"Int8\",\n",
    "    \"CancellationCode\": \"category\",\n",
    "    \"Distance\": \"float32\",\n",
    "    \"Flights\": \"Int8\",\n",
    "}\n",
    "\n",
    "dfs = [pd.read_csv(f, low_memory=False, usecols=usecols, dtype=dtypes) for f in files]\n",
    "\n",
    "flights = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flights), flights.memory_usage(deep=True).sum() / (1024**3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b88aa",
   "metadata": {},
   "source": [
    "#### Find TranStats terminology at:\n",
    "[BTS Table Info](https://www.transtats.bts.gov/TableInfo.asp?gnoyr_VQ=FGJ&QO_fu146_anzr=b0-gvzr&V0s1_b0yB=D)\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "BTS column names are in all caps, with mixed formatting, so we'll strip out white space and special characters and replace spaces and hyphens with underscores for standard Python-friendly formatting.\n",
    "\n",
    "Additionally, these BTS tables report on cancelled and diverted flights. This data will distort KPIs like average delay and on-time rates because they do not have meaningful arrival delay values. Therefore, we will filter them out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbb6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "\n",
    "flights.columns = (\n",
    "    flights.columns.str.strip()  # Remove leading/trailing whitespace\n",
    "        .str.lower()  # Lowercase all characters\n",
    "        .str.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing flags as 0/1, removing any NaNs that may interfere with filtering\n",
    "\n",
    "flag_cols = [\"cancelled\", \"diverted\", \"depdel15\", \"arrdel15\", \"flights\"]\n",
    "for c in flag_cols:\n",
    "    flights[c] = flights[c].fillna(0).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Cancelled/Diverted Flights\n",
    "\n",
    "flights_clean = flights[\n",
    "    (flights['cancelled'] == 0) &\n",
    "    (flights['diverted'] == 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"Rows before cleaning: {len(flights)}\")\n",
    "print(f\"Rows after cleaning: {len(flights_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794f395",
   "metadata": {},
   "source": [
    "<mark>personal note</mark>\n",
    "\n",
    ".copy() uses a lot of RAM. Flights currently uses ~8G memory, which pushes to 16G when executing above code. If more issues arise due to memory overhead (other apps open and using RAM), then use this code to filter with a mask, selected only needed columns, before you copy:\n",
    "\n",
    "mask = (flights[\"cancelled\"] == 0) & (flights[\"diverted\"] == 0)\n",
    "flights_clean = flights.loc[mask].copy()\n",
    "\n",
    "print(f\"Rows before cleaning: {len(flights):,}\")\n",
    "print(f\"Rows after cleaning:  {len(flights_clean):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e53581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete flights dataframe to free up memory (and prevent crashes during modeling)\n",
    "del flights\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa4106",
   "metadata": {},
   "source": [
    "### Time-based features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduled Departure Hour\n",
    "flights_clean['dep_hour'] = (flights_clean['crsdeptime'].floordiv(100).astype('int8'))\n",
    "\n",
    "# Weekend Flag\n",
    "flights_clean['is_weekend'] = flights_clean['dayofweek'].isin([6, 7]).astype('int8')\n",
    "\n",
    "# Route feature (control size of feature by using categories to save memory)\n",
    "flights_clean['route'] = (flights_clean['origin'].astype(str) + \"-\" + flights_clean['dest'].astype(str)).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check feature creation\n",
    "flights_clean[['arrdel15', 'dep_hour', 'is_weekend', 'route', 'distance']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory check\n",
    "flights_clean.memory_usage(deep=True).sum() / (1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61fc465",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "#### Dataset sanity check first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for distribution, range of hours, distance scale\n",
    "flights_clean[['arrdel15', 'dep_hour', 'is_weekend', 'route', 'distance']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e1b48",
   "metadata": {},
   "source": [
    "#### Class balance: how many flights are late?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_rate = flights_clean['arrdel15'].mean()\n",
    "late_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab09049",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_clean[\"arrdel15\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c52c2",
   "metadata": {},
   "source": [
    "19.1% of flights were late, or nearly 1 in 5--normal for airline data. Use as baseline difficulty\n",
    "\n",
    "#### Delay rate by departure hour (strongest predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_delay = (\n",
    "    flights_clean\n",
    "        .groupby('dep_hour', observed=True)['arrdel15']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .sort_values(by='dep_hour')\n",
    ")\n",
    "\n",
    "hourly_delay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hourly delay rate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hourly_delay['dep_hour'], hourly_delay['arrdel15'], marker='o')\n",
    "plt.title('Arrival Delay Rate by Scheduled Departure Hour')\n",
    "plt.xlabel('Scheduled Departure Hour')\n",
    "plt.ylabel('Late Arrival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499367cb",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. morning flights have lowest delay rate\n",
    "2. delay risks climbs throughout the day\n",
    "\n",
    "#### Weekend v Weekday effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_delay = (\n",
    "    flights_clean\n",
    "        .groupby('is_weekend', observed=True)['arrdel15']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "weekend_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 0/1 for clarity\n",
    "weekend_delay['day_type'] = weekend_delay['is_weekend'].map({0: 'Weekday', 1: 'Weekend'})\n",
    "weekend_delay[['day_type', 'arrdel15']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f033a",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. lower delay rates on the weekends\n",
    "2. feature useful, but not as strong as time of day\n",
    "\n",
    "#### Carrier-level delay performance (top 10 by volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b985e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 carriers by flight count\n",
    "top_carriers = (\n",
    "    flights_clean[\"iata_code_reporting_airline\"]\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "        .index\n",
    ")\n",
    "\n",
    "# calculate average arrival delay rate for top carriers\n",
    "carrier_delay = (\n",
    "    flights_clean[flights_clean[\"iata_code_reporting_airline\"].isin(top_carriers)]\n",
    "    .groupby(\"iata_code_reporting_airline\", observed=True)[\"arrdel15\"]\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "carrier_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783dde4",
   "metadata": {},
   "source": [
    "#### Route-level delay (top 10 busiest routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aae4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 routes by flight count\n",
    "top_routes = (\n",
    "    flights_clean[\"route\"]\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "        .index\n",
    ")\n",
    "\n",
    "top_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average arrival delay rate for top routes\n",
    "route_delay = (\n",
    "    flights_clean[flights_clean[\"route\"].isin(top_routes)]\n",
    "    .groupby(\"route\", observed=True)[\"arrdel15\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "route_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335345e2",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. routes that systematically underperform: Chicago‚ÜíLa Guardia ‚Ü∫, Los Angeles‚ÜíSan Francisco ‚Ü∫\n",
    "2. are route-level features useful in model?\n",
    "\n",
    "#### Distance v delay\n",
    "sanity check--long flights have buffer to absorb delays, so short flights should show high volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_bins = pd.cut(flights_clean['distance'], bins=[0, 500, 1000, 1500, 2000, 2500, 3000, 4000, 5000])\n",
    "\n",
    "distance_delay = (\n",
    "    flights_clean\n",
    "        .groupby(distance_bins, observed=True)['arrdel15']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "distance_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_delay = (\n",
    "    flights_clean\n",
    "        .groupby(distance_bins, observed=True)['arrdelay']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "distance_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb647b08",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "I can see the following breakdown in delay rates:\n",
    "- short-haul (0 - 500): ~18.2%\n",
    "- mid-haul (500-1500): ~19-20%\n",
    "- long-haul (1500-3000): ~20%\n",
    "- very long-haul (3000+): ~16.2%\n",
    "long-haul flights have built-in schedule buffers, priority handling, reduced connection dependencies that reduce delayed arrival probability. The lower rate is expected. Short flights must handle faster turn around, congested airspace, and gate delays that have greater affect on crossing the 15 minute threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volatility for short vs long hauls actually shows up in the stdev of delay rates\n",
    "distance_delay_stdev = (\n",
    "    flights_clean\n",
    "        .groupby(distance_bins, observed=True)['arrdelay']\n",
    "        .std()\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "distance_delay_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_delay_iqr = (\n",
    "    flights_clean\n",
    "        .groupby(distance_bins, observed=True)['arrdelay']\n",
    "        .quantile([0.25, 0.75])\n",
    "        .unstack()\n",
    ")\n",
    "\n",
    "distance_delay_iqr[\"iqr\"] = distance_delay_iqr[0.75] - distance_delay_iqr[0.25]\n",
    "distance_delay_iqr.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e1c1d",
   "metadata": {},
   "source": [
    "#### Memory cleanup\n",
    "...or, next time upgrade to the bigger memory package! üßòüèª‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "del hourly_delay, weekend_delay, carrier_delay, route_delay, distance_delay, distance_delay_iqr, distance_delay_stdev\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de896e8",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "#### Logistics Regression\n",
    "\n",
    "**Target:** `arrdel15` (0 = on time, 1 = ‚â•15 min late)\n",
    "\n",
    "**Baseline Features:** `dep_hour`, `is_weekend`, `distance`, `iata_code_reporting_airline` (EDA above proved that these matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "model_features = [\n",
    "    'dep_hour',\n",
    "    'is_weekend',\n",
    "    'distance',\n",
    "    'iata_code_reporting_airline'\n",
    "]\n",
    "\n",
    "X = flights_clean[model_features].copy()\n",
    "Y = flights_clean['arrdel15']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Y # keeps late/on-time ratio consistent across train/test splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e091cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "numeric_features = ['dep_hour', 'distance'] # scaled\n",
    "binary_features = ['is_weekend'] # no scaling needed\n",
    "categorical_features = ['iata_code_reporting_airline'] # one-hot encoded due to categorical nature\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('bin', 'passthrough', binary_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            max_iter=1000, \n",
    "            class_weight='balanced'\n",
    "        )),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Evaluation\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(Y_test, Y_proba)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = (\n",
    "    model.named_steps['preprocessor']\n",
    "        .get_feature_names_out()\n",
    ")\n",
    "\n",
    "coefs = model.named_steps['classifier'].coef_[0]\n",
    "\n",
    "coef_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefs\n",
    "    })\n",
    "    .sort_values('coefficient', ascending=False) \n",
    ")\n",
    "\n",
    "coef_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e249f",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Decision Tree-model\n",
    "\n",
    "**Target:** `arrdel15` (0 = on time, 1 = ‚â•15 min late)\n",
    "\n",
    "**Baseline Features:** `dep_hour`, `is_weekend`, `distance`, `iata_code_reporting_airline` (EDA above proved that these matter)\n",
    "\n",
    "*Why Gradient Boosting?* Gradient Boosting has better performance on structure data and uses less memory (compared with Random Forest). `HistGradientBoostingClassifier` is fast, scales well, handles large datasets, and does not require one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature set for tree-modeling\n",
    "X_tree = flights_clean[model_features].copy()\n",
    "Y_tree = flights_clean['arrdel15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26035da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features (Ordinal)\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_tree['iata_code_reporting_airline'] = encoder.fit_transform(\n",
    "    X_tree[['iata_code_reporting_airline']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for tree model\n",
    "X_tree_train, X_tree_test, Y_tree_train, Y_tree_test = train_test_split(\n",
    "    X_tree,\n",
    "    Y_tree,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Y_tree\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618bc17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting Classifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "gb_model = HistGradientBoostingClassifier(\n",
    "    max_depth=6, # avoids overfitting\n",
    "    learning_rate=0.05, # slower learning for better generalization\n",
    "    max_iter=150,\n",
    "    class_weight='balanced', # handle class imbalance and consistent with baseline (previous lr model)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_tree_train, Y_tree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7841e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Tree Model\n",
    "\n",
    "Y_tree_pred = gb_model.predict(X_tree_test)\n",
    "Y_tree_proba = gb_model.predict_proba(X_tree_test)[:, 1]\n",
    "\n",
    "print(classification_report(Y_tree_test, Y_tree_pred))\n",
    "print(f\"Tree Model ROC AUC Score: {roc_auc_score(Y_tree_test, Y_tree_proba)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LR vs Tree Model\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Gradient Boosting'],\n",
    "    'ROC AUC Score': [\n",
    "        roc_auc_score(Y_test, Y_proba), \n",
    "        roc_auc_score(Y_tree_test, Y_tree_proba)\n",
    "    ],\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdaa7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Tree Model\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# use sample of test set for efficiency\n",
    "# n=min(200000, len(X_tree_test))\n",
    "sample_idx = X_tree_test.sample(n=200_000, random_state=42).index\n",
    "\n",
    "perm = permutation_importance(\n",
    "    gb_model, \n",
    "    X_tree_test.loc[sample_idx], \n",
    "    Y_tree_test.loc[sample_idx],\n",
    "    n_repeats=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf13b52",
   "metadata": {},
   "source": [
    "perm_importance_df = ({\n",
    "    'feature': X_tree.columns,\n",
    "    'importance': perm.importances_mean\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "perm_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5c7c3",
   "metadata": {},
   "source": [
    "#### Model Comparison\n",
    "- logistics regression model was used as an interpretable baseline\n",
    "- gradient boosting classifier was trained to capture nonlinear and interaction effects\n",
    "- both models were trained on identical features (`model_features`) and evaluated on the same test set.\n",
    "\n",
    "#### Results Summary\n",
    "- logistic regression provides strong interpretability and stable performance\n",
    "- gradient boosting improves ROC AUC by 1.7 points, indicating a better ranking of delay risk (thousands of flights from the 14.6 million in our dataset)\n",
    "- the strongest predictor in both models is scheduled departure hour, confirming operational delay propogation.\n",
    "\n",
    "#### **Recommendation**\n",
    "- use **logistic regression** for explainability and policy decisions\n",
    "- use **gradient boosting** for operational risk scoring where performance is a priority\n",
    "- both models agree on key drivers, which increases confidence in insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd894ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e046e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2ff14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d849dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6b593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda: datasci311",
   "language": "python",
   "name": "datasci311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
